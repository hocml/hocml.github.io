---
hide_sidebar: true
title:  "Bài học cay đắng - The bitter lession"
categories: incompleteideas
permalink: 2019-08-05-the-bitter-lession-post.html
tags: [new AI, machinelearning, AI, mathematics]
---

**Rich Sutton**

**March 13, 2019**

Bài học lớn nhất có được từ 70 năm nghiên cứu về AI đó là, các phương pháp chung tận dụng khả năng tính toán của phần cứng là những phương pháp hiệu quả nhất và cải thiện được độ chính xác lớn nhất. Lý do chính cho điều này là [định luật Moore](https://vi.wikipedia.org/wiki/%C4%90%E1%BB%8Bnh_lu%E1%BA%ADt_Moore), tổng quát của định luật Moorse nói việc về chi phí tiếp tục giảm theo cấp số nhân trên mỗi đơn vị tính toán. Hầu hết các nghiên cứu về AI đã được thực hiện với một mặc định rằng khả năng tính toán có sẵn cho tác nhân (thuật toán, mô hình học, ..v.v) là không đổi (trong trường hợp đó tận dụng kiến ​​thức của con người sẽ là một trong những cách duy nhất để cải thiện hiệu suất) NHƯNG, với một thời gian dài hơn so với một dự án nghiên cứu điển hình( mình nghĩ tác giả muốn nói đến thời gian để  huấn luyện, tinh chỉnh siêu tham số, thử sai ..v.v.), nguồn lực tính toán lớn hơn chắc chắn trở nên có sẵn. Tìm kiếm một sự cải tiến tạo ra sự khác biệt trong ngắn hạn, các nhà nghiên cứu tìm cách tận dụng kiến ​​thức của con người về  lĩnh vưc nào đó (ý là đưa kiến thức của con người vào cho máy), nhưng điều duy nhất quan trọng trong dài hạn là phải tận dụng được nguồn lưc tính toán. Cả hai điều này không nhất thiết phải phát triển đối lập nhau, nhưng trong thực tế, chúng có xu hướng xảy ra theo chiều hướng đối lập nhau. Thời gian dành cho cho một trong những việc này thì sẽ không còn thời gian danh cho việc còn lại. Chúng ta có tâm lý dành nhiều thời gian hơn cho một trong những việc này. Và phương pháp tiếp cận bằng tri thức của con người có xu hướng làm cho những phương pháp này trở nên phức tạp, khiến chúng trở nên không phù hợp với những phương pháp tận dụng nguồn được lực tính toán. Đã có rất nhiều trường hợp về những bài học cay đắng này mà những nhà nghiên cứu AI sau đó mới nhận ra, và việc xem xét một số trường hợp nổi bật nhất có thể sẽ giúp ích cho chúng ta.

Trong trò chơi cờ vua trên máy tính, những phương pháp đã đánh bại nhà vô địch thế giới, Kasparov, vào năm 1997, dựa trên việc thực hiện phương pháp tìm kiếm "vét cạn". Vào thời điểm đó, phần lớn các nhà nghiên cứu cờ vua trên máy tính đã theo đuổi các phương pháp thúc đẩy sự hiểu biết của con người về cấu trúc đặc biệt của cờ vua. Khi một cách tiếp cận dựa trên phương pháp tìm kiếm đơn giản với phần cứng và phần mềm đặc biệt tỏ ra hiệu quả hơn rất nhiều, các nhà nghiên cứu cờ vua dựa trên tri thức con người này là những người thua cuộc nhưng họ đã không làm gì đối với thất bại này. Họ nói rằng tìm kiếm "một cách vét cạn - bruce force-" có thể đã chiến thắng, nhưng đó không phải là một phương pháp tổng quát, và đó thực sự không phải là cách con người chơi cờ. Các nhà nghiên cứu này muốn các phương pháp dựa trên đầu vào là kiến thức của con người để chiến thắng và họ đã thất vọng khi họ không làm được như họ mong muốn.

Một ví dụ tương tự khác là về quá trình nghiên cứu là về máy tính Go, cái khác duy nhất là công trình này lại bị trì hoãn thêm 20 năm nữa. Rất nhiều nỗ lực ban đầu đã đành cho việc tránh sử dụng phương pháp tìm kiếm bằng cách tận dụng kiến thức của con người, hoặc các đặc điểm nổi bật của trò chơi, nhưng tất cả những nỗ lực đó đã chứng minh là không liên quan tới việc nâng cao khả năng cuả "máy", hoặc tệ hơn, một khi phương pháp tìm kiếm đã được áp dụng hiệu quả ở quy mô. Một điều quan trọng nữa là việc "học" bằng cách tự chơi để "học" được một hàm mục tiêu (như trong nhiều trò chơi khác và ngay cả trong cờ vua, mặc dù việc học không đóng vai trò lớn trong chương trình máy tính năm 1997, chương trình máy tính lần đầu tiên đánh bại một nhà vô địch cờ vua thế giới). "Học" bằng cách tự chơi, và "học" nói chung, giống như phương pháp tìm kiếm ở chỗ nó cho phép phương pháp tận dụng được nguồn lực tính toán lớn. Tìm kiếm và "học" là hai lớp kỹ thuật quan trọng nhất tận dụng được nguồn lưc tính toán của máy. Trong máy tính Go, cũng như trong cờ vua máy tính, nỗ lực ban đầu của các nhà nghiên cứu đều hướng đến việc tận dụng sự hiểu biết của con người (để cần ít tìm kiếm hơn) và chỉ một thời gian ngay sau đó, thành công lai thuộc về những phướng phép kết hợp được khả năng tìm kiếm và "học".

Trong bài toán nhận dạng giọng nói, đã có một cuộc thi từ rất sớm, được tài trợ bởi DARPA, vào những năm 1970. Những người tham gia bao gồm cả những người là tác giả của các phương pháp đặc biệt tận dụng hiểu biết của con người về lĩnh vực ngôn ngữ --- kiến ​​thức về từ ngữ, âm vị, về giọng nói của con người, v.v. Mặt khác là những phương pháp mới hơn có tính chất thiên thống kê và tính toán nhiều hơn, dựa trên các mô hình hidden Markov models (HMM). Một lần nữa, các phương pháp thống kê đã chiến thắng các phương pháp dựa trên tri thức của con người. Điều này dẫn đến một sự thay đổi lớn trong tất cả quá trình phát triển của xử lý ngôn ngữ tự nhiên, dần dần trong nhiều thập kỷ, nơi thống kê và tính toán đã thống trị lĩnh vực này. Sự gia tăng gần đây của mạng học sâu trong nhận dạng giọng nói là bước đi gần đây nhất theo hướng này. Phương pháp học sâu phụ thuộc ít hơn vào kiến ​​thức của con người, và sử dụng nhiều tính toán hơn, cùng với việc "học" trên các bộ dữ liệu huấn luyện khổng lồ, để tạo ra các hệ thống nhận dạng giọng nói tốt hơn đáng kể. Như trong các trò chơi, các nhà nghiên cứu luôn cố gắng tạo ra các hệ thống hoạt động theo cách mà các nhà nghiên cứu nghĩ rằng trí óc của họ hoạt động --- họ đã cố gắng đưa kiến ​​thức đó vào hệ thống của họ --- nhưng cuối cùng nó đã phản tác dụng và lãng phí thời gian của những nhà nghiên cứu theo hướng này, thông qua định luật Moore, có thể thấy tính toán lớn đã trở nên có sẵn và một phương tiện đã được sinh ra để tận dụng tốt nguồn lực này.

Trong thị giác máy tính, đã có một ví dụ tương tự. Các phương pháp ban đầu được hình thành trong thị giác máy tính là phương pháp tìm kiếm các đặc trưng góc cạnh trong ảnh, hoặc đặc trưng SIFT. Nhưng ngày nay tất cả những phướng pháp đã bị loại bỏ. Các mạng nơ-ron học sâu hiện đại chỉ sử dụng các khái niệm mạng "tích chập" và bất biến với một số biến đổi nhất định (theo mình đó là phép xoay, tịnh tiến, phép scale v.v.), và cho kết quả tốt hơn rất nhiều.

Điểm chung có thể  rút ra từ những bài học cay đắng này là tri thức của não bộ có thể rất phức tạp, vô cùng phức tạp; chúng ta nên ngừng cố gắng tìm kiếm những cách đơn giản để hiểu được nội dung của não bộ, chẳng hạn như những cách đơn giản để suy nghĩ về không gian, những đối tượng, đa tác nhân hoặc [đối xứng](https://en.wikipedia.org/wiki/Symmetry) -- tính đối xứng hiểu chung là sau khi biến đổi đối tượng như thế nào ta vẫn có được đối tượng ban đầu --. Tất cả những điều này là một phần của thế giới bên ngoài không quy luật và phức tạp. Chúng không phải là những thứ nên được phát triển và xây dựng, vì sự phức tạp của chúng là vô tận; thay vào đó chúng ta chỉ nên xây dựng các phương pháp hiểu quả có thể tìm kiếm và nắm bắt được sự phức tạp không có quy luật này. Điều cớ bản của các phương pháp này là chúng có thể tìm thấy các xấp xỉ tốt, nhưng việc tìm kiếm chúng phải bằng phương pháp của chúng ta chứ không phải là do chúng ta (nghĩa là không áp dụng tri thức sẵn có của con người). Chúng ta muốn chủ thể AI có thể khám phá tri thức như cách chúng ta khám phá, chứ không chứa những gì chúng ta đã khám phá. Xây dựng những kết quả của quá trình con người khám phá tri thức chỉ làm cho nó khó khăn hơn để  hiểu được quá trình khám phá tri thức được thực hiện như thế nào.

__________________________________________________________________________________________________________________________

nguồn [Richard S. Sutton](http://incompleteideas.net/)




